---
title: "DATA621 -  Spring2020 - Homework5"
author: "Abdelmalek Hajjam / Monu Chacko"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(ggplot2)    # plotting
library(dplyr)      # data manipulation
library(gridExtra)  # display
library(knitr)      # display
library(kableExtra) # display
library(mice)       # imputation
library(caTools)    # train-test split
library(MASS)       # boxcox
library(Metrics)    # rmse
library(caret)      # confusion matrix
library(VIM)        # plotting NAs
library(ggfortify)  # plotting lm diagnostic
library(car)        # VIF
library(pander)
library(pscl)       # zero-inflated model
library(DataExplorer)

opts_chunk$set(echo = TRUE)
options(scipen=999)
```

## Problem Statement

In this homework assignment, you will explore, analyze and model a data set containing information on approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine. These cases would be used to provide tasting samples to restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a wine to be sold at a high end restaurant. A large wine manufacturer is studying the data in order to predict the number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales. 

Your objective is to build a count regression model to predict the number of cases of wine that will be sold given certain properties of the wine. HINT: Sometimes, the fact that a variable is missing is actually predictive of the target. You can only use the variables given to you (or variables that you derive from the variables provided).

## Data Exploration

The data set includes 12,795 observations with 15 variables (including target variable). Of all 15 columns, 0 are discrete, 15 are continuous, and 0 are all missing. There are 8,200 missing values out of 191,925 data points.
Below we'll display a few basic EDA techniques to gain insight into our wine dataset.

#### Basic statistics and Summary of Variables

```{r echo=FALSE, warning=FALSE}
training <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework5/wine-training-data.csv", na.strings=c("","NA"))
colnames(training)[1] <- "INDEX"

```

The data set includes 14 independent variables: 

- `AcidIndex`: Proprietary method of testing total acidity of training by using a weighted average.
- `Alcohol`: Alcohol content of training.
- `Chlorides`: Chloride content of training.
- `CitricAcid`: Citric acid content of training.
- `Density`: Density of training.
- `FixedAcidity`: Fixed Acidity of training.
- `FreeSulfurDioxide`: Sulfur dioxide content of training.
- `LabelAppeal`: Marketing score indicating the appeal of label design for consumers.
- `ResidualSugar`: Residual sugar of training.
- `STARS`: training rating by a team of experts. Ranges from 1 (Poor) to 4 (Excellent) stars.
- `Sulphates`: Sulfate content of training.
- `TotalSulfurDioxide`: Total sulfur dDioxide of training.
- `VolatileAcidity`: Volatile acid content of training.
- `pH`: pH of training

Dependent variable is `TARGET` representing number of cases of training purchased.

\newpage
The table below shows summary of all variables. 

```{r echo=FALSE, warning=FALSE}
sumtbl = data.frame(Variable = character(),
                    Class = character(),
                    Min = integer(),
                    Median = integer(),
                    Mean = double(),
                    SD = double(),
                    Max = integer(),
                    Num_NAs = integer(),
                    Num_Zeros = integer(),
                    Num_Neg = integer())
for (i in c(3:16)) {
  sumtbl <- rbind(sumtbl, data.frame(Variable = colnames(training)[i],
                                     Class = class(training[,i]),
                                     Min = min(training[,i], na.rm=TRUE),
                                     Median = median(training[,i], na.rm=TRUE),
                                     Mean = mean(training[,i], na.rm=TRUE),
                                     SD = sd(training[,i], na.rm=TRUE),
                                     Max = max(training[,i], na.rm=TRUE),
                                     Num_NAs = sum(is.na(training[,i])),
                                     Num_Zeros = length(which(training[,i]==0)),
                                     Num_Neg = sum(training[,i]<0 & !is.na(training[,i]))))
}
colnames(sumtbl) <- c("Variable", "Class", "Min", "Median", "Mean", "SD", "Max", 
                      "Num of NAs", "Num of Zeros", "Num of Neg Values")
pander(sumtbl[,1:7])
pander(sumtbl[,c(1,8:10)])
```

The Variables `LabelAppeal`, `AcidIndex` and `STARS` are categorical, and represented by numeric values in logical order. We Will then use them in modeling as numeric variables. All other variables are continous.

\newpage
#### Missing Values

We have 8 variables that have some sort of `NA` values. Most variables have negative values. 
Below plots show how the missing values are spread out within the data set. Approximately 25% of observations are missing a `STARS` value, and approximately 9% of observations have  missing values.

```{r echo=FALSE, warning=FALSE}
aggr_plot <- aggr(training, col=c('lightblue','darkred'), 
                  numbers=FALSE, sortVars=TRUE, labels=names(training), 
                  cex.axis=.7, gap=3, 
                  ylab=c("Missing data - Histogram","Pattern"))
```

\newpage
#### Exploratory Plots

Checking on the variables with different plots reveal that distribution for all variables are symmetrical and unimodal.
We use scatter plot, density plot and box plot to inspect the variables. Box plots are also similar for most of the variables (excluding the last category).


We will use the variable `Chlorides` as an example for plotting. The plots will be the same for the other variables.

```{r echo=FALSE, warning=FALSE, fig.width=10, fig.height=10}
mvariable <- "Chlorides"
ploting.data <- as.data.frame(cbind(training[, mvariable], training$TARGET)); colnames(ploting.data) <- c("X", "Y")
box.plot <- ggplot(ploting.data, aes(x = 1, y = X)) + stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
histogram.plot <- ggplot(ploting.data, aes(x = X)) + geom_histogram(aes(y=..density..), bins=50, colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(X, na.rm=TRUE)), color="blue", linetype="dashed", size=1)
scotter.plot <- ggplot(ploting.data, aes(x=X, y=Y)) + geom_point() + xlab("Scatterplot") + ylab("")
box.plot.target <- ggplot(training, aes(x = as.factor(TARGET), y = Chlorides)) + stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplots per Target (Number of training Cases)") + ylab("Chlorides") + theme(axis.ticks.x=element_blank())
grid.arrange(box.plot, histogram.plot, scotter.plot, box.plot.target, layout_matrix=rbind(c(1,2,2),c(1,3,3),c(4,4,4)))
```

\newpage
#### Correlations

Correlation matrix below shows that there is very little correlation between variables. All can contribute in the modeling. 

```{r echo=FALSE, warning=FALSE}
Correlation.matrix <- cor(training[,2:16], use="pairwise.complete.obs")
Correlation.matrix <- round(Correlation.matrix, 2)
rownames(Correlation.matrix)[7:8] <- c("FreeSO2", "TotalSO2")
colnames(Correlation.matrix)[7:8] <- c("FreeSO2", "TotalSO2")
Correlation.matrix.out <- as.data.frame(Correlation.matrix) %>% mutate_all(function(z) {
  cell_spec(z, "latex", color = ifelse(z>0.5 | z<(-0.5),"blue","black"))
})
rownames(Correlation.matrix.out) <- colnames(Correlation.matrix.out)
Correlation.matrix.out %>%
  kable("latex", escape = F, align = "c", row.names = TRUE) %>%
  kable_styling("striped", full_width = F, font_size = 6) %>%
  row_spec(0, angle = 90)
```

#### Dependent Variable

The dependent variable `TARGET` ranges from 0 (no cases purchased) to 8 cases of training purchased. The most common outcome is 4 cases at 25% of all observations followed closely with no purchase (0 cases) at 21%. Not counting the 0 outcome, it seems that the variable has unimodal, symmetrical distribution resembling normal distibution centered around 4.

```{r echo=FALSE, warning=FALSE}
outcome <- as.data.frame(table(training$TARGET))
outcome <- cbind(outcome, as.data.frame(round(table(training$TARGET)/sum(table(training$TARGET)),2))[,2])
colnames(outcome) <- c("Outcome", "# of Observations", "Percent of Total")
pander(outcome)
```

\newpage
## Data Preparation

All variables, as is, are good for modeling. Our dataset require some few transformations. The only concern we should have are the missing and negative values.

#### Consider Missing Values

We have 2 variables, `LabelAppeal` and `AcidIndex`, that do not have any missing values. these are good to go.

The `STARS` variable has 3,359 missing values. It represents experts' rating and has meaning in modeling. We will replace the missing values for `STARS` with 0.

The Other variables that have missing values will be imputed. We will use the R package called `mice`. It has a handy method called `norm` that will perform the imputation. 

```{r echo=FALSE, warning=FALSE}
plot_missing(training)
```


#### Consider Negative Values 

The `Alcohol` variable has about 118 observations with negative values. Such a thing cannot be possible, because 0 is the minimum value for being non-alcoholic. Therefore we will transform this variable by taking absolute value for its observations.

Other variables contain negative values, but this must be how they chose to represent the data. Therefore, except for `STARS`, all negative values for all variables will not be changed and will stay the same.

#### Training/Testing Split

We split our Dataset into a training (75% of observations) and testing (25% of observations) portions. We use the `caTools` R package to do that, based on the `TARGET` variable so that the target classes can be random, not bias, and having the same porportion between the 2 sets.

```{r echo=FALSE, warning=FALSE, results="hide"}
training$STARS[is.na(training$STARS)] <- 0
training$Alcohol <- abs(training$Alcohol)

trainingImputed <- mice(training, m=5, maxit=10, meth='norm', seed=500)
training <- complete(trainingImputed)

# Split into train and validation sets
set.seed(88)
split <- sample.split(training$TARGET, SplitRatio = 0.75)
training.TRAIN <- subset(training, split == TRUE)
training.TEST <- subset(training, split == FALSE)
```

\newpage
## Modeling: Linear

We used two linear models, one full including all variables and another one using the `stepAIC` function. 

The first resulted in R^2 of 0.5268, RMSE of 1.3184 and accuracy in predicting the outcomes in the testing set of 0.2853. 

The second model who used the `stepAIC` function resulted in R^2 of 0.5266, RMSE of 1.3193 and accuracy of 0.2847 against the testing set. 

According to those metrics, we concluded that the full model performed very slightly better than the stepwise model. 

Here is the model summary.

```{r echo=FALSE, warning=FALSE}
lmModel <- lm(TARGET ~ .-INDEX,data = training.TRAIN)
summary(lmModel)
```

The diagnostic plots show us that the model performs really well. Because the dependent variable is a count variable, Some of the plots are not very useful for this dataset.

```{r echo=FALSE, warning=FALSE}
autoplot(lmModel)
```

According to the below confusion matrix, the accuracy is fairly low at 28.3%; however, if we examine full confusion matrix below we can see that the model mostly errors only by 1 or 2 cases. There may be significant cost associated with this error.

```{r echo=FALSE, warning=FALSE}
pred <- predict(lmModel, newdata=training.TEST)
predRound <- as.factor(round(pred,0))
levels(predRound) <- levels(as.factor(training.TEST$TARGET))
confusionMatrix(predRound, as.factor(training.TEST$TARGET))

```

## Modeling: Poisson

The linear model seemed to perform good with all variables. So for the poisson regression similar strategy was applied - a model with all variables and a model optimized by the stepwise method. There was no considerable imrovement using the stepwise method, so for comparison reasons below is the summary for the full model. RMSE for this model is 1.39855, slightly worse than for the linear model.

```{r echo=FALSE, warning=FALSE}
poisson.Model <- glm (TARGET ~ .-INDEX, data = training.TRAIN, family = poisson)
summary(poisson.Model)
pred <- predict(poisson.Model, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
```

Comparing predicted values to the test data, with the folowing confusion matrix, shows that the model does not predict _no purchase_ outcome (count is 0). Worse than that, it often predicts fewer cases than the test data indicates. Accuracy is very bad and lower than the one for the linear model.

```{r echo=FALSE, warning=FALSE}
pred <- predict(poisson.Model, newdata=training.TEST, type='response')
predRound <- as.factor(round(pred,0)-1)
testData <- as.factor(training.TEST$TARGET)
levels(predRound) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "0")
levels(testData) <- c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
confusion.Matrix <- confusionMatrix(predRound, testData)
confusion.Matrix$overall[1]
confusion.Matrix$table
```

## Modeling: Negative Binomial

We create a negative binomial model with all variables with the help of the `MASS` R package. This model turned out to be nearly identical to the poisson model. code is on the bottom of the page. 

Both models exhibited over-dispersion. 

```{r echo=FALSE, warning=FALSE}
negativeBinomial.Model <- glm.nb(TARGET ~ .-INDEX, data = training.TRAIN)
summary(negativeBinomial.Model)
```

## Modeling: Zero-Inflated Negative Binomial

Poisson and negative binomial models do not account for the 0 outcome. So a zero-inflated negative binomial model was attempted using the `pscl` R package. RMSE for this model is 1.2727, the best one out of all models.

```{r echo=FALSE, warning=FALSE}
ZeroInflated.Model <- zeroinfl(TARGET ~ .-INDEX, data = training.TRAIN, dist = "negbin")
summary(ZeroInflated.Model)

pred <- predict(ZeroInflated.Model, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
```

This model has the accuracy of 36.03%, again the best one out of all models. It predicts 0 outcomes (not ideally, but perhaps it can be improved with more research).

```{r echo=FALSE, warning=FALSE}
pred <- predict(ZeroInflated.Model, newdata=training.TEST, type='response')
predRound <- as.factor(round(pred,0))
testData <- as.factor(training.TEST$TARGET)
confusion.Matrix <- confusionMatrix(predRound, testData)
confusion.Matrix$overall[1]
confusion.Matrix$table
```

\newpage
## Model Selection

Considering log-likelihood of all models, it is clear that zero-inflated negative binomial model is the best option. More research in that direction will probably be beneficial.

```{r echo=FALSE, warning=FALSE}
ll <- rbind(logLik(lmModel), logLik(poisson.Model))
ll <- rbind(ll, logLik(negativeBinomial.Model))
ll <- rbind(ll, logLik(ZeroInflated.Model))
ll <- as.data.frame(ll)
rownames(ll) <- c("Linear", "Poisson", "NB", "ZINB")
colnames(ll) <- c("Log-Likelihood")
ll[, "DF"] <- c(16,15,16,31)
pander(ll)
```

Comparing all coefficients using full model with all methods, we see that usually the coefficients are similar in sign and in magnitude. 
We also notice that between NB and ZINB models, some small coefficients do change signs. 

```{r echo=FALSE, warning=FALSE}
coef <- as.data.frame(lmModel$coefficients)
coef <- cbind(coef, as.data.frame(poisson.Model$coefficients))
coef <- cbind(coef, as.data.frame(negativeBinomial.Model$coefficients))
coef <- cbind(coef, as.data.frame(ZeroInflated.Model$coefficients))
coef <- round(coef, 6)
colnames(coef) <- c("Linear", "Poisson", "NB", "ZINB (Count)", "ZINB (Zero)")
pander(coef[,1:4])
```

\newpage
## Evaluation of our selected Zero-Inflated Model

We will only display the evaluation result of the first 50 observations from the evaluation set. The generated Prediction file will be saved into a csv file called Prediction_for_Eval.csv located at: https://github.com/theoracley/Data621/blob/master/Homework5/Prediction_For_Eval.csv
```{r echo=FALSE, warning=FALSE, results="hide"}
eval <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework5/wine-evaluation-data.csv",na.strings=c("","NA"))
colnames(eval)[1] <- "INDEX"

eval$STARS[is.na(eval$STARS)] <- 0
eval$Alcohol <- abs(eval$Alcohol)

evalImputed <- mice(eval, m=5, maxit=10, meth='norm', seed=500)
eval <- complete(evalImputed)
```

```{r echo=FALSE, warning=FALSE}
pred <- predict(ZeroInflated.Model, newdata=eval, type="response")
results <- eval[, c("INDEX")]
results <- cbind(results, prob=round(pred,4))
results <- cbind(results, predict=round(pred,0))
colnames(results) <- c("Index", "Predicted Value", "Predicted Outcome")
pander(head(results, 50))
write.csv(results, "Prediction_For_Eval.csv")
```

\newpage
## APPENDIX 

```{r eval=FALSE}
library(ggplot2)    # plotting
library(dplyr)      # data manipulation
library(gridExtra)  # display
library(knitr)      # display
library(kableExtra) # display
library(mice)       # imputation
library(caTools)    # train-test split
library(MASS)       # boxcox
library(Metrics)    # rmse
library(caret)      # confusion matrix
library(VIM)        # plotting NAs
library(ggfortify)  # plotting lm diagnostic
library(car)        # VIF
library(pander)
library(pscl)       # zero-inflated model
library(DataExplorer)

training <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework5/wine-training-data.csv", na.strings=c("","NA"))
colnames(training)[1] <- "INDEX"

# Basic statistic
nrow(training); ncol(training)
summary(training)

# Summary table
sumtbl = data.frame(Variable = character(),
                    Class = character(),
                    Min = integer(),
                    Median = integer(),
                    Mean = double(),
                    SD = double(),
                    Max = integer(),
                    Num_NAs = integer(),
                    Num_Zeros = integer(),
                    Num_Neg = integer())
for (i in c(3:16)) {
  sumtbl <- rbind(sumtbl, data.frame(Variable = colnames(training)[i],
                                     Class = class(training[,i]),
                                     Min = min(training[,i], na.rm=TRUE),
                                     Median = median(training[,i], na.rm=TRUE),
                                     Mean = mean(training[,i], na.rm=TRUE),
                                     SD = sd(training[,i], na.rm=TRUE),
                                     Max = max(training[,i], na.rm=TRUE),
                                     Num_NAs = sum(is.na(training[,i])),
                                     Num_Zeros = length(which(training[,i]==0)),
                                     Num_Neg = sum(training[,i]<0 & !is.na(training[,i]))))
}
colnames(sumtbl) <- c("Variable", "Class", "Min", "Median", "Mean", "SD", "Max", 
                      "Num of NAs", "Num of Zeros", "Num of Neg Values")
pander(sumtbl[,1:7])
pander(sumtbl[,c(1,8:10)])


# Categorical variables
table(training$LabelAppeal)
table(training$AcidIndex)
table(training$STARS)

# Exploratory plots
mvariable <- "FixedAcidity"
mvariable <- "VolatileAcidity"
mvariable <- "CitricAcid"
mvariable <- "ResidualSugar"
mvariable <- "Chlorides"
mvariable <- "FreeSulfurDioxide"
mvariable <- "TotalSulfurDioxide"
mvariable <- "Density"
mvariable <- "pH"
mvariable <- "Sulphates"
mvariable <- "Alcohol"
mvariable <- "LabelAppeal" 
mvariable <- "AcidIndex"
mvariable <- "STARS"
mvariable <- "Chlorides"
ploting.data <- as.data.frame(cbind(training[, mvariable], training$TARGET)); colnames(ploting.data) <- c("X", "Y")
box.plot <- ggplot(ploting.data, aes(x = 1, y = X)) + stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplot") + ylab("") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
histogram.plot <- ggplot(ploting.data, aes(x = X)) + geom_histogram(aes(y=..density..), bins=50, colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") + ylab("") + xlab("Density Plot with Mean") +
  geom_vline(aes(xintercept=mean(X, na.rm=TRUE)), color="blue", linetype="dashed", size=1)
scotter.plot <- ggplot(ploting.data, aes(x=X, y=Y)) + geom_point() + xlab("Scatterplot") + ylab("")
box.plot.target <- ggplot(training, aes(x = as.factor(TARGET), y = Chlorides)) + stat_boxplot(geom ='errorbar') + geom_boxplot() + 
  xlab("Boxplots per Target (Number of training Cases)") + ylab("Chlorides") + theme(axis.ticks.x=element_blank())
grid.arrange(box.plot, histogram.plot, scotter.plot, box.plot.target, layout_matrix=rbind(c(1,2,2),c(1,3,3),c(4,4,4)))


# Correlation matrix
Correlation.matrix <- cor(training[,2:16], use="pairwise.complete.obs")
Correlation.matrix <- round(Correlation.matrix, 2)
rownames(Correlation.matrix)[7:8] <- c("FreeSO2", "TotalSO2")
colnames(Correlation.matrix)[7:8] <- c("FreeSO2", "TotalSO2")
Correlation.matrix.out <- as.data.frame(Correlation.matrix) %>% mutate_all(function(z) {
  cell_spec(z, "latex", color = ifelse(z>0.5 | z<(-0.5),"blue","black"))
})
rownames(Correlation.matrix.out) <- colnames(Correlation.matrix.out)
Correlation.matrix.out %>%
  kable("latex", escape = F, align = "c", row.names = TRUE) %>%
  kable_styling("striped", full_width = F, font_size = 6) %>%
  row_spec(0, angle = 90)


# Dependent variable
outcome <- as.data.frame(table(training$TARGET))
outcome <- cbind(outcome, as.data.frame(round(table(training$TARGET)/sum(table(training$TARGET)),2))[,2])
colnames(outcome) <- c("Outcome", "# of Observations", "Percent of Total")
pander(outcome)


# IMPUTATION / TRANSFORMATION
plot_missing(training)

# Imputation
training$STARS[is.na(training$STARS)] <- 0
training$Alcohol <- abs(training$Alcohol)
trainingImputed <- mice(training, m=5, maxit=10, meth='norm', seed=500)
training <- complete(trainingImputed)

# Split into train and test sets
set.seed(88)
split <- sample.split(training$TARGET, SplitRatio = 0.75)
training.TRAIN <- subset(training, split == TRUE)
training.TEST <- subset(training, split == FALSE)


# LINEAR MODEL

# All variables
lmModel <- lm(TARGET ~ .-INDEX,data = training.TRAIN)
summary(lmModel)


# stepAIC
lmModel <- stepAIC(lmModel, trace=FALSE, direction='both')
summary(lmModel)
# Model returned by step AIC
lmModel <- lm(TARGET ~ VolatileAcidity + CitricAcid + 
                Chlorides + FreeSulfurDioxide + 
                TotalSulfurDioxide + Sulphates + Alcohol + 
                LabelAppeal + AcidIndex + STARS,
              data = training.TRAIN)
summary(lmModel)
# Manual variations
lmModel <- lm(TARGET ~ VolatileAcidity + Chlorides + 
                FreeSulfurDioxide + 
                TotalSulfurDioxide + Sulphates + Alcohol + 
                LabelAppeal + AcidIndex + STARS,
              data = training.TRAIN)
summary(lmModel)
lmModel <- lm(TARGET ~ VolatileAcidity + Chlorides + 
                FreeSulfurDioxide + 
                TotalSulfurDioxide + Alcohol + 
                LabelAppeal + AcidIndex + STARS,
              data = trainingTRAIN)
summary(lmModel)

# Calculate RMSE
pred <- predict(lmModel, newdata=training.TEST)
rmse(training.TEST$TARGET, pred)

# Confusion matrix
predRound <- as.factor(round(pred,0))
table(predRound)
levels(predRound) <- levels(as.factor(training.TEST$TARGET))
confusionMatrix(predRound, as.factor(training.TEST$TARGET))

autoplot(lmModel)

# Model plots
plot(lmModel$residuals, ylab="Residuals")
abline(h=0)

plot(lmModel$fitted.values, lmModel$residuals, 
     xlab="Fitted Values", ylab="Residuals")
abline(h=0)

qqnorm(lmModel$residuals)
qqline(lmModel$residuals)

# POISSON and NB REGRESSION MODEL

# Poisson 1
poisson.Model <- glm (TARGET ~ .-INDEX, data = training.TRAIN, family = poisson)
summary(poisson.Model)
pred <- predict(poisson.Model, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
predRound <- as.factor(round(pred,0))
testData <- as.factor(training.TEST$TARGET)
levels(predRound) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "0")
levels(testData) <- c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
confusionMatrix(predRound, testData)

# Poisson 2
poisson.Model2 <- stepAIC(poisson.Model, trace=FALSE, direction='both')
summary(poisson.Model2)
pred <- predict(poisson.Model2, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
predRound <- as.factor(round(pred,0))
testData <- as.factor(training.TEST$TARGET)
levels(predRound) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "0")
levels(testData) <- c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
confusionMatrix(predRound, testData)

# Poisson 3
poisson.Model3 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                 Sulphates + Alcohol + LabelAppeal + 
                 AcidIndex + STARS, family = poisson, data = training.TRAIN)
summary(poisson.Model3)
pred <- predict(poisson.Model3, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
predRound <- as.factor(round(pred,0))
testData <- as.factor(training.TEST$TARGET)
levels(predRound) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "0")
levels(testData) <- c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
confusionMatrix(predRound, testData)

# NB
negativeBinomial.Model <- glm.nb(TARGET ~ .-INDEX, data = training.TRAIN)
summary(negativeBinomial.Model)
pred <- predict(negativeBinomial.Model, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
predRound <- as.factor(round(pred,0))
testData <- as.factor(training.TEST$TARGET)
levels(predRound) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "0")
levels(testData) <- c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
confusionMatrix(predRound, testData)

# Zero Inflated
ZeroInflated.Model <- zeroinfl(TARGET ~ .-INDEX, data = training.TRAIN, dist = "negbin")
summary(ZeroInflated.Model)
pred <- predict(ZeroInflated.Model, newdata=training.TEST, type='response')
rmse(training.TEST$TARGET, pred)
predRound <- as.factor(round(pred,0))
testData <- as.factor(training.TEST$TARGET)
confusionMatrix(predRound, testData)

# Deviance residuals
anova(poisson.Model, test="Chisq")
anova(poisson.Model2, test="Chisq")
anova(poisson.Model3, test="Chisq")
anova(zrModel, test="Chisq")

# VIF
vif(poisson.Model)
vif(negativeBinomial.Model)
vif(ZeroInflated.Model)

# Coefficients
coef <- as.data.frame(lmModel$coefficients)
coef <- cbind(coef, as.data.frame(poisson.Model$coefficients))
coef <- cbind(coef, as.data.frame(negativeBinomial.Model$coefficients))
coef <- cbind(coef, as.data.frame(ZeroInflated.Model$coefficients))

# Prediction
eval <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework5/wine-evaluation-data.csv",na.strings=c("","NA"))
colnames(eval)[1] <- "INDEX"

sumtbl = data.frame(Variable = character(),
                    Class = character(),
                    Min = integer(),
                    Median = integer(),
                    Mean = double(),
                    SD = double(),
                    Max = integer(),
                    Num_NAs = integer(),
                    Num_Zeros = integer(),
                    Num_Neg = integer())
for (i in c(3:16)) {
  sumtbl <- rbind(sumtbl, data.frame(Variable = colnames(eval)[i],
                                     Class = class(eval[,i]),
                                     Min = min(eval[,i], na.rm=TRUE),
                                     Median = median(eval[,i], na.rm=TRUE),
                                     Mean = mean(eval[,i], na.rm=TRUE),
                                     SD = sd(eval[,i], na.rm=TRUE),
                                     Max = max(eval[,i], na.rm=TRUE),
                                     Num_NAs = sum(is.na(eval[,i])),
                                     Num_Zeros = length(which(eval[,i]==0)),
                                     Num_Neg = sum(eval[,i]<0 & !is.na(eval[,i]))))
}
colnames(sumtbl) <- c("Variable", "Class", "Min", "Median", "Mean", "SD", "Max", 
                      "Num of NAs", "Num of Zeros", "Num of Neg Values")
sumtbl

eval$STARS[is.na(eval$STARS)] <- 0
eval$Alcohol <- abs(eval$Alcohol)

evalImputed <- mice(eval, m=5, maxit=10, meth='norm', seed=500)
eval <- complete(evalImputed)

pred <- predict(ZeroInflated.Model, newdata=eval, type="response")
results <- eval[, c("INDEX")]
results <- cbind(results, prob=round(pred,4))
results <- cbind(results, predict=round(pred,0))
colnames(results) <- c("Index", "Predicted Value", "Predicted Outcome")
pander(head(results, 100))

#Write the results to a Prediction file
write.csv(results, "Prediction_For_Eval.csv")
```
