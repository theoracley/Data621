---
title: "Data621- HW3"
author: " Group: Abdelmalek Hajjam / Monu Chacko"
date: "3/18/2020"
output: 
  pdf_document:
      highlight: tango
      toc: true
      toc_depth: 4
      number_sections: true
      df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, warning=FALSE, error=FALSE, cache=FALSE, results='hide', message=FALSE}
library(knitr)
library(random)
library(corrplot)
library(car)
library(caret)
library(lmtest)
library(PerformanceAnalytics)
library(pROC)
library(psych)
library(dplyr)
library(reshape)
library(stringr)
library(tibble)
```

## Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels.

You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided).

## Description

Let's look at our variables of interest in our dataset are:

```{r, echo=FALSE}
crime_variables <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework3/CrimeVariables.csv", header=TRUE,  sep = ",") 
crime_variables
```

# Data Exploration

## Reading the data

The crime dataset in composed of 2 csv files, one for training our data and the other one is for evaluation. We are reading them from our github repository.

```{r}
training_data <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework3/crime-training-data_modified.csv", header=TRUE,  sep = ",") 

evaluation_data <- read.csv("https://raw.githubusercontent.com/theoracley/Data621/master/Homework3/crime-evaluation-data_modified.csv", header=TRUE,  sep = ",")
```

## Our pattern

We will be guided by the following pattern all along, predicting the target variable using every explanatory variable. The following is one example of such a variable:

```{r}
glm.tr <- glm(target ~ ptratio, data = training_data)
summary(glm.tr)
```
Not to say anything for this example output at this time, but just to mention that the predicted model will include $\beta_0 = -0.55998$ for the intercept and $\beta_1 = 0.05715$ for the rate of change.

Visualize this example:

```{r, echo=FALSE}
plot(training_data$target ~ training_data$ptratio,
      type = "p", 
      col = "blue",
      main = paste("'High crime' vs 'Pupil-teacher ratio by town'"),
      xlab = 'Pupil-teacher ratio by town', 
      ylab = "High crime")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
boxplot(ptratio ~ target, 
        data = training_data, 
        notch = FALSE,
        col = (c("darkgreen","darkred")),
        main = "Pupil-teacher ratio by town", 
        xlab ="High crime")
```

From that simple example we could make some inferences such as it seems that the higher the *Pupil-teacher ratio by town* could influence in *High crime*; this could make sense in the real world since teachers aren't able to provide more individualized education techniques when group sizes are bigger, thus reducing quality education time per student. But yet again, this is just an example on how one predictor could influence in this particular case.

## general exploration

Let's get deeper with our data and try to get any insights we can. So Let's go!

### Dimensions

Our data has the folowing dimensions.

```{r, echo=FALSE}
dimensions <- dim(training_data)
dimensions <- data.frame('Records' = dimensions[1],
                         'Variables' = dimensions[2])
dimensions
```

It looks like our data has `r dimensions$Records[1]` records and `r dimensions$Variables[1]` variables including the **target** variable corresponding to *high crime*.

### Structure

Let's investigate our dataset and take a look at its structure.

```{r, echo=FALSE}
str(training_data)
```

### Summary

Let's look at the summary statistics about our data.

```{r, echo=FALSE}
summary_crime <- data.frame(unclass(summary.default(training_data)))
summary_crime
```

Now, let's look at all the variables closely,including the target variable, and try to get insight from them.

```{r, echo=FALSE}
summary_crime <- data.frame(unclass(summary(training_data)), 
                          check.names = FALSE, 
                          row.names = NULL,
                          stringsAsFactors = FALSE)

# transposing the resulting data frame
summary_crime <- data.frame(t(summary_crime))
# renaming the columns
colnames(summary_crime) <- c('Min', '1st Qu', 'Median', 'Mean', '3rd Qu', 'Max')
# extract the numeric values
summary_crime$Min <- as.numeric(gsub('Min.   :', '', summary_crime$Min))
summary_crime$`1st Qu` <- as.numeric(gsub('1st Qu.:', '', summary_crime$`1st Qu`))
summary_crime$Median <- as.numeric(gsub('Median :', '', summary_crime$Median))
summary_crime$Mean <- as.numeric(gsub('Mean   :', '', summary_crime$Mean))
summary_crime$`3rd Qu` <- as.numeric(gsub('3rd Qu.:', '', summary_crime$`3rd Qu`))
summary_crime$Max <- as.numeric(gsub('Max.   :', '', summary_crime$Max))
summary_crime
```

### Missing data

According to the statistics above, there are no missing values or **NA**, since missing data was not reported above.

### Visualizations

In the below graphs, the colors indicate that any record not including a high crime shows a green circle, while a record indicating a high crime has been plot in a red triangle. The diagonal plots the empirical distribution for both classes.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
scatterplotMatrix(~ zn + indus + chas + nox | target, data=training_data,
                  span=0.7, id.n=0, col =  c("darkgreen","darkred"))
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
scatterplotMatrix(~ rm + age + dis + rad | target, data=training_data,
                  span=0.7, id.n=0, col =  c("darkgreen","darkred"))
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
scatterplotMatrix(~ tax + ptratio + lstat + medv | target, data=training_data,
                  span=0.7, id.n=0, col =  c("darkgreen","darkred"))
```

Let's separate our data for visualization purposes.

```{r, echo=FALSE}
nrows <- 1
ncols <- 4 
boxcol <- c("darkgreen","darkred")
par(mfrow=c(nrows,ncols))
boxplot(training_data[,1] ~ training_data[,13], col = boxcol, main = colnames(training_data)[1], xlab = 'Crime')
boxplot(training_data[,2] ~ training_data[,13], col = boxcol, main = colnames(training_data)[2], xlab = 'Crime')
boxplot(training_data[,3] ~ training_data[,13], col = boxcol, main = colnames(training_data)[3], xlab = 'Crime')
boxplot(training_data[,4] ~ training_data[,13], col = boxcol, main = colnames(training_data)[4], xlab = 'Crime')
```

```{r, echo=FALSE}
nrows <- 1
ncols <- 4 
boxcol <- c("darkgreen","darkred")
par(mfrow=c(nrows,ncols))
boxplot(training_data[,5] ~ training_data[,13], col = boxcol, main = colnames(training_data)[5], xlab = 'Crime')
boxplot(training_data[,6] ~ training_data[,13], col = boxcol, main = colnames(training_data)[6], xlab = 'Crime')
boxplot(training_data[,7] ~ training_data[,13], col = boxcol, main = colnames(training_data)[7], xlab = 'Crime')
boxplot(training_data[,8] ~ training_data[,13], col = boxcol, main = colnames(training_data)[8], xlab = 'Crime')
```

```{r, echo=FALSE}
nrows <- 1
ncols <- 5 
boxcol <- c("darkgreen","darkred")
par(mfrow=c(nrows,ncols))
boxplot(training_data[,9] ~ training_data[,13], col = boxcol, main = colnames(training_data)[9], xlab = 'Crime')
boxplot(training_data[,10] ~ training_data[,13], col = boxcol, main = colnames(training_data)[10], xlab = 'Crime')
boxplot(training_data[,11] ~ training_data[,13], col = boxcol, main = colnames(training_data)[11], xlab = 'Crime')
boxplot(training_data[,12] ~ training_data[,13], col = boxcol, main = colnames(training_data)[12], xlab = 'Crime')

```

### Count values

Let's have a small understanding on how many records were categorized as 0 and how many as 1.

```{r, echo=FALSE}
trainingCount <- training_data %>% group_by(target) %>% summarize(Counts=n())
trainingCount <- data.frame(trainingCount)
trainingCount$Percent <- trainingCount$Counts / sum(trainingCount$Counts)
round(trainingCount,3)
```

From the above results, we could assume that in effect the values seems to be uniformly distributed since almost half the data represent 0 and almost half represent 1.


### Correlations

Let's create some visualizations for the correlation matrix.

```{r, echo=FALSE}
# Need to reorder by columns in order to set 'target' first
training_data <- training_data[c(13,1:12)]
```

```{r,echo=FALSE}
my_matrix <- training_data
cor_res <- cor(my_matrix, use = "na.or.complete")
```

#### Graphical visualization

```{r, warning=FALSE, echo=FALSE}
corrplot(cor_res, 
         type = "upper", 
         order = "original", 
         tl.col = "black", 
         tl.srt = 45, 
         tl.cex = 0.75)
```

#### Numerical visualization

```{r, echo=FALSE, warning=FALSE}
chart.Correlation(training_data,
                  method="spearman",
                  histogram=TRUE,
                  pch=16)
```

From the above graphs, we can easily identify some strong correlations in between the response variable `target` and other variables.

Get more insights from the Correlations table.

```{r, echo=FALSE}
cor_res <- data.frame(cor_res)
cor_res[1]
```

As we can easily check the above results, there seems to have considerable correlations in between our `target` variable among other given variables.

Something interesting to note from the above graph, is that we can easily visualize some sort of strong positive correlation in between variables; for example: `tax` seems to be strongly positively correlated to `ptratio`. In this case, their correlation values will be: `r cor_res['rad','tax']`, something to keep in mind in case of multivariate co-linearity.


# DATA PREPARATION

From the correlations table, we could focus on the variables that contain the strongest correlations related to our `target` variable; in this case, I will set my cut off at with any correlation in which the absolute value will be higher than 0.5.

```{r, echo=FALSE}
cor_res1 <- cor_res[which(abs(cor_res$target) > 0.50),]
cor_res1[1]
```

As we can see, we have reduced our number of possible predictor in half. From now on, I will focus on these variables only. Notice how in this smaller table `ptratio` is not part of it? In this case, I will assume this to be correct avoiding co-linearity problems further down.


```{r, echo=FALSE}
include <- rownames(cor_res1)
reduced_Training <- training_data[include] 
```

Let's recap our previous plots for those variables.

```{r,echo=FALSE}
nrows <- 1
ncols <- 3 
boxcol <- c("darkgreen","darkred")
par(mfrow=c(nrows,ncols))
boxplot(reduced_Training[,2] ~ reduced_Training[,1], col = boxcol, main = colnames(reduced_Training)[2], xlab = 'Crime')
boxplot(reduced_Training[,3] ~ reduced_Training[,1], col = boxcol, main = colnames(reduced_Training)[3], xlab = 'Crime')
boxplot(reduced_Training[,4] ~ reduced_Training[,1], col = boxcol, main = colnames(reduced_Training)[4], xlab = 'Crime')
```

```{r,echo=FALSE}
nrows <- 1
ncols <- 3 
boxcol <- c("darkgreen","darkred")
par(mfrow=c(nrows,ncols))
boxplot(reduced_Training[,5] ~ reduced_Training[,1], col = boxcol, main = colnames(reduced_Training)[5], xlab = 'Crime')
boxplot(reduced_Training[,6] ~ reduced_Training[,1], col = boxcol, main = colnames(reduced_Training)[6], xlab = 'Crime')
boxplot(reduced_Training[,7] ~ reduced_Training[,1], col = boxcol, main = colnames(reduced_Training)[7], xlab = 'Crime')
```

Let's recap the structure of the remaining variables:

```{r}
str(reduced_Training)
```

At this point, we are getting ready to start building models, however I would like to point out that in this case is a little bit difficult to determine what data transformation could be used in order to refine our models.

# Binary Logistic Regression

We would like to point that since this work requires **Binary Logistic Regression**, we are going to be using the **logit** function as our Likelihood link function for Logistic Regression by assuming that it follows a binomial distribution as follows:

$$y_i | x_i \sim Bin(m_i,\theta(x_i))$$

so that,

$$P(Y_i=y_i | x_i)= \binom{m_i}{y_i} \theta(x_i)^{y_i}(1-\theta(x_i))^{m_i-y_i} $$

Now, in order to solve our problem, we need to build a linear predictor model in which the individual predictors that compose the response $Y_i$ are all subject to the same $q$ predictors $(x_{i1}, …, x_{iq})$. Please note that the group of predictors, are commonly known as **covariate classess**. In this case, we need a model that describes the relationship of $x_1, …, x_q$ to $p$. In order to solve this problem, we will construct a linear predictor model as follows:

$$\mathfrak{N}_i = \beta_0 + \beta_1x_{i1}+...+\beta_qx_{iq} $$

## Logit link function

In this case, since we need to set $\mathfrak{N}_i = p_i$; with $0 \le p_i \le 1$, I will use the *link function* $g$ such that $\mathfrak{N}_i = g(p_i)$ with $0 \le g^{-1}(\mathfrak{N}) \le 1$ for any $\mathfrak{N}$. In order to do so, I will pick the **Logit** link function $\mathfrak{N} = log(p/(1 - p))$.

An alternate way will be by employing the $\chi^2$ Chi square distribution; for the purposes of this project, I will employ the use of the binomial distribution or the $\chi^2$ depending on which one is a better choice, also I will assume that all $Y_i$ are all independent of each other.


# BUILD MODELS

We will use the following methods in order to build our model.

## NULL Model

In this section, we will build a **Binary Logistic Regression** Null model utilizing all the variables and data. This model will be considered to be valid and will be modified as we advance.

```{r}
Model_NULL <- glm(target ~ 1, 
              data = training_data, 
              family = binomial(link ="logit"))
summary(Model_NULL)
```

We consider this to be a valid model.

\newpage

## FULL Model

In this section I will build a **Binary Logistic Regression** Full model utilizing all the variables and data, please note that I won't do any transformations. This model will be considered to be valid and will be considered as we advance.

```{r}
Model_FULL <- glm(target ~ ., 
              data = training_data, 
              family = binomial(link ="logit"))
summary(Model_FULL)
```

Some variables are not statistically significant. But, we will assume that this is a valid model.

## STEP Procedure

Here, we will create multiple models. Here we go!

```{r}
Model_STEP <- step(Model_NULL,
                   scope = list(upper=Model_FULL),
                   direction="both",
                   test="Chisq",
                   data=training_data)
Model_STEP
```

From the above possible models, it was concluded that the Model with the lowest **Akaike's Information Criterion (AIC)** is the one containing the following variables: **nox, rad, tax, ptratio, age, medv, dis, zn**.

\newpage

### ANOVA results

From the results above, we can see the ANOVA table.

```{r}
Model_STEP$anova
```

**Nota Bene:** 

If we check our theory, the **AIC** defines as follows: *the smaller the value for AIC the better the model*; in this case, we can easily observe that just by adding certain variables, our AIC values decrease making it a better model.

## AIC Model

From the above, we conclude that the best model is as follows:

```{r}
Model_AIC = glm(formula = target ~ 
                nox + rad + tax + ptratio + age + medv + dis + zn +lstat, 
                family = binomial(link = "logit"), 
                data = training_data)
summary(Model_AIC)
```

From the above model, it is interesting to note how all of the predictor variables but `lstat` are statistically significant; also, we can notice how the Median is near zero and how the standard error could be considered low.

## Modified AIC

From the above results, we will create a new modified model by excluding `lstat` from the previous model.

```{r}
Model_AIC = glm(formula = target ~ 
                nox + rad + tax + ptratio + age + medv + dis + zn,
                family = binomial(link = "logit"), 
                data = training_data)
summary(Model_AIC)
```

Worthy to note that all predictors are statistically significant, the standard errors and the median are still small but it seems that actually increased alongside the AIC with a slight increase.


## Intuition Model

According to the correlations table, some variables are more correlation to `target` than others. In this section, we will create a model based on that output by including the following variables only and we will use it in order to choose our best selected model.

```{r,echo = FALSE}
data.frame(Variables = row.names(cor_res1))
```

In this case, we will employ the following variables: **indus, nox, age, dis, rad, tax**.

```{r}
Model_INTUITION <- glm(target ~ indus + nox + age + dis + rad + tax, 
                       data = training_data, 
                       family = binomial(link = logit))
summary(Model_INTUITION)
```

From the above, we can see that `indus` and `dis` are not statistically significant. Also, we notice how the AIC value has increased in a moderate way, along side the Residual Deviance, which is not good. So let's **refine** this model.


## Intuition Model Refined

Here, we'll conside backward elimination. So let's exclude the variables `indus` and `dis`.

```{r}
Model_Refined <- glm(target ~ nox + age + rad + tax, 
              data = training_data, 
              family = binomial(link = logit))
summary(Model_Refined)
```

We notice how all the given predictors are statistically significant but the AIC has increased, the Median is higher than before and the residual deviance also increased.


# MODEL SELECTION

From the above possible models, we will select the model given with the lowest AIC; if it is true, it includes the highest number of variables, it is the model that provides better possible outcome in this particular case; hence my selected model will be the one containing the following variables: **nox, rad, tax, ptratio, medv, age, dis, zn**.

```{r}
Model_FINAL <- Model_AIC
summary(Model_FINAL)
```

How we choose our final model this way, because:

- This model returned the lowest **Akaike's Information Criterion** AIC.

- This model returned the nearest to zero median value.

- This model included the most number of significant statistically predictive values.

- This model displayed the smallest standard errors for the considered predictor variables.

- This model present the smallest rate of change for all predictor variables.

- This model returned the lowest residual deviance.

- From the below table we can see how the probability of being higher than the $\chi^2$ are very low.

```{r}
Anova(Model_FINAL, type="II", test="Wald")
```

## Test model

From the above chosen model, I will create a reduced data frame containing only the variables needed in order to run our model.

```{r}
select_var <- c('target', 'nox', 'rad', 'tax', 'ptratio', 
                'medv', 'age', 'dis', 'zn', 'lstat')
training_data.final <- training_data[select_var]
```

### Final Model Comparisons

From here, we will define a null model with the chosen variables in order to compare results with the final model.

```{r}
Model_NULL = glm(target ~ 1,
                 data=training_data.final,
                 family = binomial(link="logit"))
summary(Model_NULL)
```

### Analysis of Deviance Table

Let's display a Deviance analysis by employing the $\chi^2$ test.

```{r}
anova(Model_FINAL,
      Model_NULL,
      test="Chisq")
```

In the above results, we can easily compare our Residual Deviance in which our model has better results compared to the null model since the null model's deviance will increase in units compared to our final model. 

### Likelihood ratio test

In order to do so, we will employ the **lrtest** function from the **lmtest** library; this is a generic function for carrying out likelihood ratio tests. The default method can be employed for comparing nested (generalized) linear models. 

```{r}
lrtest(Model_FINAL)
```

in our Final Model, we obtain much better results compared to our NULL model, hence this corroborates that our Final Model has a much better Likelihood ratio compared to the NULL Model.

### Plot of standardized residuals

The below plot shows our fitted models vs the deviance r standardized residuals.

```{r}
plot(fitted(Model_FINAL),
     rstandard(Model_FINAL),
     main = 'Standarize residuals for binary data',
     xlab = 'Fitted values',
     ylab = 'Standarized Deviance Residuals',
     col = 'blue')
```

### Simple plot of predictions

This visual is a representation of the predicted values versus the given values aka `target`.

```{r}
training_data.final$predict = predict(Model_FINAL,
                                    type="response")
plot(target ~ predict,
     data = training_data.final,
     pch = 16,
     xlab="Predicted probability of 1 response",
     ylab="Actual response",
     col = 'blue')
```

## Evaluations

In this section, we will proceed to evaluate our chosen final model in terms of (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. 

In order to do so, we will need to perform some transformations to round the given probabilities to zero decimals.

```{r}
training_data.final$predicted_target <- round(training_data.final$predict,0)
training_data_table <- table(training_data.final$predicted_target, 
                           training_data.final$target,   
                           dnn = c("Predicted", "Target"))
data.frame(training_data_table)
```

### Confusion Matrix

Let's start by building a confusion matrix in order to obtain valuable insights.

```{r}
cMatrix <- confusionMatrix(data = as.factor(training_data.final$predicted_target),
                           reference = as.factor(training_data.final$target),
                           positive = '1')
cMatrix
```

From the above results, we obtain as follows:

```{r, echo=FALSE}
data.frame(Value = cMatrix$byClass)
```

### ROC and AUC

As we know, the **Receiver Operating Characteristic Curves** (ROC) is a great quantitative assessment tool of the model. In order to quantify our model, we will employ as follows:


```{r}
# First, let's prepare our function
rocCurve <- roc(target ~ predict, data = training_data.final)
# Let's plot our RCO curve.
plot(rocCurve, print.auc=TRUE, legacy.axes = TRUE)
```

Let's see our confidence intervals.

```{r, echo=FALSE}
rownames_ci <- c('Lower bound', 'Estimated value', 'Higher bound')
crime.ci <- data.frame(AUC = ci(rocCurve))
rownames(crime.ci) <- rownames_ci
crime.ci
```

\newpage

# PREDICTIONS

## Table

In this section, we will predict the values on the **evaluation** data set employing the **training** data set.

```{r, echo=FALSE}
prob = predict(Model_FINAL, newdata=evaluation_data, type = 'response')
evaluation_data$predicted <-round(prob,0)
evaluation_data[c(13,1:12)]
```

\newpage

## Classification and probability

In this section, we will provide a table in which the classification is reported alongside the probability for it.

```{r, echo=FALSE}
evaluation_data$probability <- round(prob,3)
evaluation_data[c('predicted','probability')]
```


# APPENDIX

Code and markdown are at:

https://github.com/theoracley/Data621/blob/master/Homework3/Abdelmalek_Hajjam_HW3.Rmd


